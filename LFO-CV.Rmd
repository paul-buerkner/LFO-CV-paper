---
title: "Approximate leave-future-out cross-validation for time series models"
author: "Paul-Christian Bürkner $^{1*}$, Jonah Gabry $^2$, & Aki Vehtari $^3$"
date: |
  $^1$ Department of Psychology, University of Münster, Germany \break
  $^2$ Institute for Social and Economic Research in Policy, Columbia University, USA \break
  $^3$ Department of Computer Science, Aalto University, Finland\break
  $^*$ Corresponding author, Email: paul.buerkner@gmail.com
abstract: |
  One of the common goals of time series analysis is to use the observed series
  to inform predictions for future observations. In the absence of any actual
  new data to predict, cross-validation can be used to estimate a model's future
  predictive accuracy, for instance, for the purpose of model comparison or
  selection. As exact cross-validation for Bayesian models is often
  computationally expensive, approximate cross-validation methods have been
  developed; most notably methods for leave-one-out cross-validation (LOO-CV).
  If the actual prediction task is to predict the future given the past, LOO-CV
  provides an overly optimistic estimate as the information from future
  observations is available to influence predictions of the past. To tackle the
  prediction task properly and account for the time series structure, we can use
  leave-future-out cross-validation (LFO-CV). Like exact LOO-CV, exact LFO-CV
  requires refitting the model many times to different subsets of the data.
  Using Pareto smoothed importance sampling, we propose a method for
  approximating exact LFO-CV that drastically reduces the computational costs
  while also providing informative diagnostics about the quality of the
  approximation.
keywords: keywords
lang: en-US
class: man
# figsintext: true
numbersections: true
encoding: UTF-8
bibliography: LFO-CV
biblio-style: apalike
output:
  bookdown::pdf_document2:
     citation_package: natbib
     keep_tex: true
     toc: false
header-includes:
   - \usepackage{amsmath}
   - \usepackage[utf8]{inputenc}
   - \usepackage[T1]{fontenc}
   - \usepackage{setspace}
   - \onehalfspacing
   - \setcitestyle{round}
   - \newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
editor_options: 
  chunk_output_type: console
---

```{r setup, cache = FALSE, include = FALSE}
knitr::opts_chunk$set(
  cache = TRUE,
  echo = FALSE,
  warning = FALSE,
  message = FALSE
)
options(knitr.kable.NA = '')
```

```{r packages, cache = FALSE, include = FALSE}
library(knitr)
library(kableExtra)
library(latex2exp)
library(tidyverse)
library(brms)
library(loo)
source("sim_functions.R")

# set ggplot theme
theme_set(bayesplot::theme_default())
colors <- unname(unlist(bayesplot::color_scheme_get()[c(6, 2)]))

# set rstan options
rstan::rstan_options(auto_write = TRUE)
options(mc.cores = max(1, parallel::detectCores() - 1))
```

```{r functions}
fmt <- function(x, digits = 1, ...) {
  format(x, digits = digits, nsmall = digits, ...)
}
```

# Introduction

A time series is a set of observations each one being recorded at a specific
time [@brockwell2002]. In statistics, a wide range of time series models has
been developed, which find application in nearly all empirical sciences [e.g.,
see @brockwell2002; @hamilton1994]. One common goal of a time series
analysis is to use the observed series to inform predictions for future.
When working with discrete time -- in which time points form
discrete set -- we will refer to the task of predicting a sequence of $M$ future
observations as $M$-step-ahead prediction ($M$-SAP). Once we have
fit a Bayesian model and can sample from the posterior predictive distribution,
it is straightforward to generate predictions as far into the future as we want.
It is also straightforward to evaluate the $M$-SAP performance of a time series
model by comparing the predictions to the observed sequence of $M$ future data
points once they become available.

It is common that we would like to estimate the future predictive performance
before we can collect the future observations.
If we have many competing models we may also need to
first decide which of the models (or which combination of the models) we should
rely on for predictions [@geisser1979; @hoeting1999; @vehtari2002; @ando2010; 
@vehtari2012]. In the absence of new data with which to evaluate predictive
performance, one general approach for evaluating a model's predictive accuracy
is cross-validation. When doing cross-validation, the data is
split into two subsets. We fit the statistical model based on the first subset
and then evaluate its predictive accuracy for the second subset. We may do this
once or many times, each time leaving out another subset.

If there is no time ordering in the data or if the focus is to assess the
non-time-dependent part of the model, we can use leave-one-out
cross-validation (LOO-CV). For a data set with $N$ observations, we refit the
model $N$ times, each time leaving out one of the $N$ observations and assessing
how well the model predicts the left-out observation. Due to the higher number 
of required refits, exact LOO-CV is computationally expensive in particular
when performing full Bayesian inference where refitting the model means
estimating a new posterior distribution rather than a point estimate. However,
we may approximate exact LOO-CV using the Pareto smoothed importance sampling
algorithm [PSIS; @vehtari2017loo; @vehtari2017psis]. PSIS-LOO-CV requires only a
single fit of the full model and comes with diagnostics for assessing the
validity of the approximation.

If there is time ordering in the data and we are interested in the predictive
performance to new future time points, leaving out only one observation at a
time allows information from the future to influence predictions of the past
(i.e., times $t+1, t+2, \ldots$ should not be used to predict time $t$). To
apply the idea of cross-validation to the $M$-SAP case, we can use
leave-*future*-out cross-validation (LFO-CV). LFO-CV does not refer to one
particular prediction task but rather to various possible cross-validation
approaches that all involve some form of prediction of future time points. Like
exact LOO-CV, exact LFO-CV requires refitting the model many times to different
subsets of the data, which is computationally expensive, in particular when
performing full Bayesian inference.

In this paper, we extend the ideas from PSIS-LOO-CV and present PSIS-LFO-CV, an
algorithm that typically only requires refitting the time-series model a small
number times. This will make LFO-CV tractable for many more realistic applications
than previously possible including time series model averaging using
stacking of predictive distributions [@yao2018].

The structure of the paper is as follows. In Section \@ref(m-sap), we introduce
the idea and various forms of $M$-step-ahead predictions and how to approximate
them using PSIS. In Section \@ref(simulations), we evaluate the accuracy of the
approximation using extensive simulations. Then, in Section \@ref(case-studies),
we provide two real world case studies. One analyzing the change in level of
Lake Huron and the other examining when the annual day of the cherry blossoms in
Kyoto, Japan, occurred, with the timeline starting in the 9th century. We end
with a discussion of the usefulness and limitations of the approach in Section
\@ref(discussion).

# $M$-step-ahead predictions {#m-sap}

Assume we have a time series of observations $y = (y_1, y_2, \ldots, y_N)$ 
and let $L$ be the _minimum_ number of observations from the series that
we will require before making predictions for future data. Depending on the
application and how informative the data are, it may not be possible to make
reasonable predictions for $y_{i}$ based on $(y_1, \dots, y_{i-1})$ until $i$ is
large enough so that we can learn enough about the time series to predict future
observations. Setting $L=10$, for example, means that we will only assess
predictive performance starting with observation $y_{11}$, so that we
always have at least 10 previous observations to condition on.

In order to assess $M$-SAP performance we would like to compute the 
predictive densities

\begin{equation}
p(y_{i+1:M} \,|\, y_{1:i}) = 
  p(y_i, \ldots, y_{i + M - 1} \,|\, y_{1},...,y_{i-1}) 
\end{equation}

for each $i \in \{L + 1, \ldots, N - M + 1\}$, where we use 
$y_{i+1:M} = (y_i, \ldots, y_{i + M - 1})$ and $y_{1:i} = (y_{1}, \ldots, y_{i-1})$ 
to shorten the notation. As a global measure of predictive accuracy, we
can use the expected log posterior density [ELPD; @vehtari2017loo], which, 
for M-SAP, can be defined as

\begin{equation}
\label{ELPD}
{\rm ELPD} = \sum_{i=L+1}^{N - M + 1} 
  \int p_t(\tilde{y}_{i+1:M}) \log p(\tilde{y}_{i+1:M} \,|\, y_{1:i})
  \, {\rm d} \, \tilde{y}_{i+1:M}.
\end{equation}

The distribution $p_t(\tilde{y}_{i+1:M})$ describes the true data generating
process for new data $\tilde{y}_{i+1:M}$. As these true data generating processes
are unknown, we approximate the ELPD using LFO-CV, which leads to
 
\begin{equation}
{\rm ELPD}_{\rm LFO} = \sum_{i=L+1}^{N - M + 1} \log p(y_{i+1:M} \,|\, y_{1:i}).
\end{equation}

The quantities $p(y_{i+1:M} \,|\, y_{1:i})$ can be computed with the help of the
posterior distribution $p(\theta \,|\, y_{1:i})$ of the parameters $\theta$
conditional on only the first $i-1$ observations of the time-series:

\begin{equation}
\label{Lpred}
p(y_{i+1:M} \,| \, y_{1:i}) = 
  \int p(y_{i+1:M} \,| \, y_{1:i}, \theta) \, 
    p(\theta\,|\,y_{1:i}) \, {\rm d} \theta. 
\end{equation}

For factorizable models, the response values are conditionally 
independent given the parameters, and the likelihood can be written 
in the factorized form

\begin{equation}
p(y \,|\, \theta) = \prod_{n=1}^N p(y_j \,|\, \theta).
\end{equation}

In this case, $p(y_{i+1:M} \,|\, y_{1:i}, \theta)$ reduces to 
\begin{equation}
p(y_{i+1:M} \,|\, y_{1:i}, \theta) =p(y_{i+1:M} \,|\, \theta) = 
\prod_{n = i}^{i + M -1} p(y_j \,|\, \theta),
\end{equation}
due to the assumption of conditional independence between $y_{i+1:M}$ and $y_{1:i}$
given $\theta$. Cross-validation for non-factorizable models, which 
does not make this assumption, is discussed in @buerkner:non-factorizable.

In practice, we will not be able to directly solve the integral in 
(\@ref(Lpred)), but instead have to use Monte-Carlo methods to approximate it.
Having obtained $S$ random draws $(\theta_{1:i}^{(1)}, \ldots, \theta_{1:i}^{(S)})$ 
from the posterior distribution $p(\theta\,|\,y_{1:i})$, we can estimate 
$p(y_{i+1:M} | y_{1:i})$ as

\begin{equation}
p(y_{i+1:M} \,|\, y_{1:i}) \approx \frac{1}{S}
\sum_{s=1}^S p(y_{i+1:M} \,|\, y_{1:i}, \theta_{1:i}^{(s)}),
\end{equation}

which further simplifies for factorizable models as shown above.

## Approximate $M$-step-ahead predictions {#approximate-MSAP}

The above equations include the posterior distributions from many
different fits of the model to different subsets of the data. To obtain
the predictive density $p(y_{i+1:M} \,|\, y_{1:i})$, a model is fit to
only the first $i-1$ data points, and we will need to do this for every value of
$i$ under consideration (i.e., all $i \in \{L + 1, \ldots, N - M + 1\}$).
Below, we will present a new algorithm to reduce the number of models that need
to be fit for the purpose of obtaining each of the densities $p(y_{i+1:M} \,|\,
y_{1:i})$. This algorithm relies in a central manner on Pareto smoothed
importance sampling [@vehtari2017loo; @vehtari2017psis], which we will briefly
review next.

### Pareto smoothed importance sampling {#psis}

In general, importance sampling is a technique to compute expectations with
respect to some target distribution using an approximating proposal distribution
that is easier to draw samples from than the actual target. If $f(\theta)$ is
the target and $g(\theta)$ is the proposal distribution, we can write any
expectation $\mathbb{E}_f[h(\theta)]$ of some function $h(\theta)$ with
respect to $f$ as

\begin{equation}
\mathbb{E}_f[h(\theta)] = \int h(\theta) f(\theta) \,d\, \theta 
 = \frac{\int [h(\theta) f(\theta) / g(\theta)] g(\theta) \,d\, \theta}
    {\int [f(\theta) / g(\theta)] g(\theta) \,d\, \theta} 
 = \frac{\int h(\theta) r(\theta) g(\theta) \,d\, \theta}
    {\int r(\theta) g(\theta) \,d\, \theta}
\end{equation}

with importance ratios 
\begin{equation}
r(\theta) = \frac{f(\theta)}{g(\theta)}.
\end{equation}

Accordingly, if $\theta^{(s)}$ are $S$ random draws from $g(\theta)$, we can
approximate

\begin{equation}
\mathbb{E}_f[h(\theta)] \approx 
\frac{\sum_{s=1}^S h(\theta^{(s)}) r(\theta^{(s)})}{\sum_{s=1}^S r(\theta^{(s)})},
\end{equation}

provided that we can compute the raw importance ratios $r(\theta^{(s)})$ up to
some multiplicative constant. We see that the raw importance ratios serve as 
weights of the corresponding random draws in the approximation of the quantity 
of interest. The main problem with this approach is that
the raw importance ratios tend to have high or infinite variance and
as such, results computed on their basis can be highly unstable. 

In order to stabilize those computations, one solution is to regularize the
largest raw importance ratios using the corresponding quantiles of generalized
Pareto distribution fitted to the largest raw importance ratios. This procedure
is called Pareto smooth importance sampling [PSIS; @vehtari2017loo;
@vehtari2017psis] and has been demonstrated to have a lower error and faster
convergence rate than other commonly used regularization techniques
[@vehtari2017psis]. In addition, PSIS comes with a useful diagnostic to evaluate
the goodness of the importance sampling approximation. The shape parameter $k$
of the generalized Pareto distribution fitted to the largest importance ratios provides
information about the number of existing moments of the weight distribution and
the actual importance sampling estimate. When $k<0.5$, the weight distribution
has finite variance, and as a result of the central limit theorem, the convergence of
the importance sampling estimate with increasing number of draws will be fast. 
This implies that approximate LOO-CV via PSIS is highly accurate for $k<0.5$ 
[@vehtari2017psis]. For $0.5 \leq k < 1$, a generalized central limit
theorem holds, but the convergence rate drops quickly when $k$ increases
[@vehtari2017psis].  In practice, PSIS has been shown to be relatively robust
for $k < 0.7$ [@vehtari2017loo; @vehtari2017psis]. As such, the default 
threshold is set to $0.7$ when performing PSIS LOO-CV [@vehtari2017loo].


### PSIS applied to $M$-step-ahead predictions

We now come back to our task of performing $M$-step-ahead predictions in
time-series models. Starting with $i = N - M + 1$, we approximate each
$p(y_{i+1:M} \,|\, y_{1:i})$ via

\begin{equation}
 p(y_{i+1:M} \,|\, y_{1:i}) \approx
   \frac{ \sum_{s=1}^S w_i^{(s)}\, p(y_{i+1:M} \,|\, \theta^{(s)})}
        { \sum_{s=1}^S w_i^{(s)}},
\end{equation}

where $w_i^{(s)}$ are the PSIS weights and $\theta^{(s)}$ are draws from the
posterior distribution based on _all_ observations. To obtain $w_i^{(s)}$, we
first compute the raw importance ratios

\begin{equation}
r_i^{(s)} = r_i(\theta^{(s)}) = \frac{f_i(\theta^{(s)})}{g(\theta^{(s)})} 
\propto \frac{
\prod_{j \in J \backslash J_i} p(y_j \,|\, \,\theta^{(s)}) p(\theta^{(s)})
}{
\prod_{j \in J} p(y_j \,|\, \,\theta^{(s)}) p(\theta^{(s)})
}
= \frac{1}{\prod_{j \in J_i} p(y_j \,|\, \,\theta^{(s)})},
\end{equation}

with $J = \{1, \ldots, N\}$, and then stabilize them using PSIS as described
above. The index set $J_i$ contains all the indices of observations which are
part of the actually fitted model but not of the model whose predictive
performance we are trying to approximate. That is, for the starting value 
$i = N - M + 1$, we have $J_i = \{i, \ldots, N\}$. This approach to computing
importance ratios is a generalization of the approach used in PSIS-LOO-CV, where
only a single observation is left out at a time and thus $J_i = i$ for all $i$.

Starting from $i = N - M + 1$, we gradually *decrease* $i$ by $1$ (i.e., we move
backwards in time) and repeat the process. At some observation $i$, the
variability of the importance ratios $r_i^{(s)}$ will become too large and
importance sampling fails. We will refer to this particular value of $i$ as
$i^\star_1$. To identify the value of $i^\star_1$, we check for which value of
$i$ does the estimated shape parameter $k$ of the generalized Pareto
distribution first cross a certain threshold $\tau$ [@vehtari2017psis]. Only
then do we refit the model using only observations before $i^\star_1$ and then
restart the process. Until the next refit, we have $J_i = \{i, \ldots, i^\star_1
-1 \}$ for $i < i^\star_1$, as the refitted model only contains the observations
up to index $N^\star_1 = i^\star_1 - 1$. An illustration of the above described 
procedure is shown in Figure \@ref(fig:vis-msap).

In some cases we may only need to refit once and in other cases we will find a
value $i^\star_2$ that requires a second refitting, maybe an $i^\star_3$ that
requires a third refitting, and so on. We repeat the refitting as many times as
is required (only if $k > \tau$) until we arrive at $i = L + 1$. Recall that $L$
is the minimum number of observations we have deemed acceptable for making
predictions (setting $L=0$ means predicting only based on the prior). A
detailed description of the algorithm in the form of pseudo code is provided 
in Appendix A. If the data contains multiple independent time-series, the 
above described algorithm should be applied to each of these time-series, 
separately, and the obtained ELPD values can be summed up afterwards.

```{r vis-msap, fig.width=8, fig.height=3, fig.cap="Visualisation of PSIS approximated one-step-ahead predictions leaving out all future values. Predicted observations are indicated by **X**. In the shown example, the model was last refit at the $i^\\star = 5$th observation."}
status_levels <- c("included", "left out", "left out (PSIS)")
df <- data.frame(
  obs = rep(1:9, 3),
  i = factor(rep(3:5, each = 9)),
  Status = c(
    rep("included", 2), rep("left out (PSIS)", 2), rep("left out", 5),
    rep("included", 3), rep("left out (PSIS)", 1), rep("left out", 5),
    rep("included", 4), rep("left out", 5)
  )
) %>%
  mutate(Status = factor(Status, levels = status_levels))

msap_colors <- c(
  bayesplot::color_scheme_get("viridis")$light,
  bayesplot::color_scheme_get("viridis")$dark,
  bayesplot::color_scheme_get("viridis")$mid_highlight
)

ggplot(df, aes(obs, i, fill = Status)) +
  geom_tile(height = 0.9, width = 1, col = "black") +
  annotate(
    'text', x = 3:5, y = c(1, 2, 3), 
    label = "X", parse = TRUE, 
    size = 10, color = "white"
  ) +
  labs(x = "Observation", y = "Predicted observation") +
  scale_x_continuous(breaks = 1:9) +
  scale_fill_manual(values = msap_colors) +
  bayesplot::theme_default() +
  NULL
```

The threshold $\tau$ is crucial to the accuracy and speed of the proposed
algorithm. If $\tau$ is too large, we need fewer refits and thus achieve higher
speed, but accuracy is likely to suffer. If $\tau$ is too small, we get high
accuracy but a lot of refits to that speed will drop noticeably. When performing
exact cross-validation of Bayesian models, almost all of the computational time is spend
fitting models, while the time needed to do predictions is negligible in
comparison. That is, a reduction of the number of refits basically implies a
proportional reduction in the overall time necessary for cross-validation of Bayesian models.
For PSIS-LFO-CV introduced in the present paper, we can expect an appropriate
threshold to be somewhere between $0.5 \leq \tau \leq 0.7$. It is unlikely to be
as high as $\tau = 0.7$ used for PSIS-LOO-CV, as the errors are more dependent
in PSIS-LFO-CV. If there is a large error leaving out $i$th observation,
then there is likely to be a large error when leaving out observations $i, i-1,
i-2, \ldots$ until a refit is performed. That is, highly influential
observations with high $k$ are likely to have stronger effects for the total
estimate in LFO-CV than in LOO-CV. We will come back to the issue of setting
appropriate thresholds in Section \@ref(simulations).

An alternative to the LFO-CV approach discussed above is to exclude only the
block of future values that directly follow the observations to be predicted
while retaining all of the more distant future values. We will discuss this 
approach in Appendix B.

# Simulations {#simulations}

To evaluate the goodness of the approximation of PSIS-LFO-CV, we performed a
simulation study by systematically varying the following conditions: The number
$M$ of future observations to be predicted took on values of $M = 1$ and $M =
4$. The threshold $\tau$ of the Pareto $k$ estimates was varied between $k =
0.5$ to $k = 0.7$ in steps of $0.1$. In addition, we evaluated six different
data generating models with linear and/or quadratic terms and/or autoregressive
terms of order 2 (see Figure \@ref(fig:simmodels)). In all
conditions, the time-series consistent of $N = 200$ observations and the minimal
number of observations to make predictions was set to $L = 25$ throughout.

```{r, include = FALSE}
seed <- 1234
set.seed(seed)
N <- 200
time <- seq_len(N)
stime <- scale_unit_interval(time)
models <- c(
  "constant", "linear", "quadratic",
  "AR2-only", "AR2-linear", "AR2-quadratic"
)

fits <- preds <- setNames(vector("list", length(models)), models)
for (m in names(fits)) {
   file <- paste0("models/fit_", m)
   fits[[m]] <- fit_model(model = m, N = N, seed = seed, file = file)
   pred <- posterior_predict(fits[[m]])
   preds[[m]] <- fits[[m]]$data %>% 
     mutate(       
       time = time, 
       stime = stime,
       Estimate = colMeans(pred), 
       Q5 = apply(pred, 2, quantile, probs = 0.05),
       Q95 = apply(pred, 2, quantile, probs = 0.95),
       model = m
    )
}
preds <- as_tibble(bind_rows(preds)) %>%
   mutate(model = factor(model, levels = models)) %>%
   select(-`I(stime^2)`)
```

```{r simmodels, fig.height=4, fig.cap="Illustration of the models used in the simulations."}
ggplot(preds, aes(x = time, y = Estimate)) +
  facet_wrap(~model) +
  geom_smooth(aes(ymin = Q5, ymax = Q95), stat = "identity", size = 0.5) +
  geom_point(aes(y = y), size = 0.5) +
  labs(y = "y")
```

Autoregressive (AR) models are some of the most commonly used time-series models. 
An AR(p) model -- an autoregressive model of order $p$ -- can be defined as

\begin{equation}
y_i = \eta_i + \sum_{k = 1}^p \varphi_k y_{i - k} + \varepsilon_i,
\end{equation}

where $\eta_i$ is the linear predictor for the $i$th observation, $\varphi_k$ are
the autoregressive parameters and $\varepsilon_i$ are pairwise independent
errors, which are usually assumed to be normally distributed with equal variance
$\sigma^2$. The model implies a recursive formula that allows for computing the
right-hand side of the above equation for observation $i$ based on the values of
the equations for previous observations. Thus, by definition, responses of
AR-models are not conditionally independent. However they are still
factorizable, that is we may write down a separate likelihood contribution per
observation [see @buerkner:non-factorizable for more discussion on 
factorizability of statistical models].

In addition to exact and approximate LFO-CV, we also compute approximate LOO-CV
for comparison. This is not because we think LOO-CV is a generally appropriate
approach for time-series models, but because, in the absence of any approximate
LFO-CV method, researchers may have used approximate LOO-CV for time-series
models in the past simply because it was available. As such, demonstrating that
LOO-CV is a biased estimate of LFO-CV underlines the importance of our newly
developed methods for approximate LFO-CV.

All simulations were done in R [@R2018] using the brms package [@brms1;
@brms2] together with the probabilistic programming language Stan
[@carpenter2017] for the modeling fitting, the loo package [@vehtari2017loo] for
the PSIS computation, and several tidyverse packages [@tidyverse] for data
processing. The full code used in the simulations as well as all results are
available on Github (https://github.com/paul-buerkner/LFO-CV-paper).

## Results {#sim_results}

```{r}
mlevels <- c(
  "constant", "linear", "quadratic",
  "AR2-only", "AR2-linear", "AR2-quadratic"
)
tau_levels <- TeX(paste0("$\\tau$ = ", c(0.5, 0.6, 0.7)))
lfo_sims <- read_rds("results/lfo_sims.rds") %>%
  as_tibble() %>%
  mutate(
    model = factor(model, levels = mlevels),
    tau = factor(k_thres, labels = tau_levels),
    elpd_loo = map_dbl(res, ~ .$loo_cv$estimates["elpd_loo", 1]),
    elpd_exact_lfo = map_dbl(res, ~ .$lfo_exact_elpd[1]),
    elpd_approx_lfo = map_dbl(res, ~ .$lfo_approx_elpd[1]),
    elpd_diff_lfo = elpd_approx_lfo - elpd_exact_lfo,
    elpd_diff_loo = elpd_loo - elpd_exact_lfo,
    npreds = map_dbl(res, ~ sum(!is.na(.$lfo_approx_elpds))),
    nrefits = lengths(map(res, ~ attr(.$lfo_approx_elpds, "refits"))),
    rel_nrefits = nrefits / npreds
  )

# create subsets
lfo_sims_1sap <- lfo_sims %>% filter(is.na(B), M == 1)
block_lfo_sims_1sap <- lfo_sims %>% filter(!is.na(B), M == 1)
lfo_sims_4sap <- lfo_sims %>% filter(is.na(B), M == 4)
block_lfo_sims_4sap <- lfo_sims %>% filter(!is.na(B), M == 4)
```

Results of the 1-SAP simulations are visualized
in Figure \@ref(fig:1sap). Comparing the columns of Figure \@ref(fig:1sap), it
is clearly visible that the accuracy of the PSIS approximation increases with
decreasing $\tau$, up to almost perfect accuracy for $\tau = 0.5$. At the same 
time, the proportion of observations at which refitting the model was required
increased substantially with decreasing $\tau$ (see Table 
\@ref(tab:refits)). Using $\tau = 0.6$ induced a slight positive
bias in PSIS-LFO-CV, but also reduced the number of required refits by roughly 
$30\%$. Another $30\%$ reduction in the number of refits was achieved by
using $\tau = 0.7$ but at the cost of disproportionally increasing the 
positive bias in PSIS-LFO-CV. As expected, LOO-CV is a biased estimate of the 
1-SAP performance for all non-constant models in particular those with a trend 
in the time-series (see light-blue histograms in Figure \@ref(fig:1sap)).

```{r 1sap, fig.height=8, fig.cap="Simulation results of 1-step-ahead predictions. Histograms are based on 100 simulation trials of time-series with $N = 200$ observations requiring at least $L = 25$ observations to make predictions."}
lfo_sims_1sap %>% 
  select(elpd_diff_lfo, elpd_diff_loo, model, tau) %>%
  gather("Type", "elpd_diff", elpd_diff_lfo, elpd_diff_loo) %>%
  ggplot(aes(x = elpd_diff, y = ..density.., fill = Type)) +
  facet_grid(
    model ~ tau, scales = "free_y", 
    labeller = label_parsed
  ) +
  geom_histogram(alpha = 0.7) +
  scale_fill_manual(
    values = colors,
    labels = c("Approximate LFO-CV", "Approximate LOO-CV")
  ) +
  labs(x = 'ELPD difference to exact LFO-CV', y = "Density") +
  geom_vline(xintercept = 0, linetype = 2) +
  theme_bw() +
  theme(legend.position = "bottom") +
  NULL
```

```{r refits, cache=FALSE}
lfo_sims %>% 
  filter(is.na(B)) %>%
  select(model, M, k_thres, rel_nrefits) %>%
  group_by(model, M, k_thres) %>%
  summarise(rel_nrefits = round(mean(rel_nrefits), 2)) %>%
  ungroup() %>%
  spread("model", "rel_nrefits") %>%
  mutate(M = ifelse(duplicated(M), "", M)) %>%
  rename(`$\\tau$` = "k_thres") %>%
  kable(
    caption = "Mean proportions of required refits.",
    booktabs = TRUE,
    escape = FALSE
  ) %>%
  footnote(
    general = "Note: Results are based on 100 simulation trials of time-series with $N = 200$ observations requiring at least $L = 25$ observations to make predictions. Abbreviations: $\\\\tau$ = threshold of the Pareto $k$ estimates; $M$ = number of predicted future observations.",
    general_title = "",
    threeparttable = TRUE,
    escape = FALSE
  )
```

Results of the 4-SAP simulations are visualized in
Figure \@ref(fig:4sap). Comparing the columns of Figure \@ref(fig:4sap), it is
clearly visible that the accuracy of the PSIS approximation increases with
decreasing $\tau$, up to almost perfect accuracy for $\tau = 0.5$. At the same
time, the proportion of observations at which refitting the model was required
increased substantially with decreasing $\tau$ (see Table \@ref(tab:refits)). In
light of the corresponding 1-SAP results (see above), this is not surprising as
the procedure to determining the necessity of a refit is independent of $M$ (see
Section \@ref(approximate-MSAP)). Using $\tau = 0.6$ again induced a slight
positive bias in PSIS-LFO-CV, but also reduced the number of required refits by
roughly $30\%$. Another $30\%$ reduction in the number of refits was achieved by
using $\tau = 0.7$ but at the cost of disproportionally increasing the positive
bias in PSIS-LFO-CV. PSIS-LOO-CV is not displayed in Figure \@ref(fig:4sap) as
the number of observations predicted as each step (4 vs. 1) renders 4-SAP LFO-CV
and LOO-CV incomparable.

```{r 4sap, fig.height=8, fig.cap="Simulation results of 4-step-ahead predictions. Histograms are based on 100 simulation trials of time-series with $N = 200$ observations requiring at least $L = 25$ observations to make predictions."}
lfo_sims_4sap %>% 
  select(elpd_diff_lfo, model, tau) %>%
  ggplot(aes(x = elpd_diff_lfo, y = ..density..)) +
  facet_grid(
    model ~ tau, scales = "free_y", 
    labeller = label_parsed
  ) +
  geom_histogram(alpha = 0.7, fill = colors[1]) +
  labs(x = 'ELPD difference of approximate and exact LFO-CV', y = "Density") +
  geom_vline(xintercept = 0, linetype = 2) +
  theme_bw() +
  theme(legend.position = "bottom") +
  NULL
```

# Case Studies {#case-studies}

## Annual measurements of the level of Lake Huron {#case-LH}

To illustrate the application of PSIS-LFO-CV for estimating expected $M$-SAP
performance, we will fit a model for 98 annual measurements of the water level
(in feet) of [Lake Huron](https://en.wikipedia.org/wiki/Lake_Huron) from the
years 1875--1972. This data set is found in the *datasets* R package, which is
installed automatically with R [@R2018]. The time-series shows rather strong
autocorrelation of the level as some trend towards lower levels for later points
in time. We fit an AR(4) model and display the model implied predictions along
with the observed values in Figure \@ref(fig:lake-huron).

```{r}
data("LakeHuron")
N <- length(LakeHuron)
df <- data.frame(
  y = as.numeric(LakeHuron),
  year = as.numeric(time(LakeHuron)),
  time = 1:N
) 
```

```{r fit_lh, results = "hide"}
fit_lh <- brm(
  y | mi() ~ 1, 
  data = df, 
  autocor = cor_ar(~time, p = 4), 
  prior = prior(normal(0, 0.5), class = "ar"),
  chains = 2, warmup = 1000, iter = 5000,
  control = list(adapt_delta = 0.99),
  seed = 5838296, file = "models/fit_lh"
)
```

```{r lake-huron, fig.cap="Water Level in Lake Huron (1875-1972). Black points are observed data. The blue line represents mean predictions of an AR(4) model with 90% prediction intervals shown in gray.", fig.height=3}
preds <- posterior_predict(fit_lh)
preds <- cbind(
  Estimate = colMeans(preds), 
  Q5 = apply(preds, 2, quantile, probs = 0.05),
  Q95 = apply(preds, 2, quantile, probs = 0.95)
)

ggplot(cbind(df, preds), aes(x = year, y = Estimate)) +
  geom_smooth(aes(ymin = Q5, ymax = Q95), stat = "identity", size = 0.5) +
  geom_point(aes(y = y)) + 
  labs(y = "Water Level (ft)", x = "Year")
```

```{r}
L <- 20
k_thres <- 0.6
loo_lh <- loo(log_lik(fit_lh)[, (L + 1):N])
```

Based on this data and model, we will illustrate the use of PSIS-LFO-CV to
provide estimates of $1$-SAP and $4$-SAP leaving out all future values. To allow
for reasonable predictions of future values, we will require at least $L = 20$
historical observations (20 years) to make predictions. Further, we set a
threshold of $\tau =$ `r k_thres` for the Pareto $k$ estimates at which define
that refitting becomes necessary. Our fully reproducible analysis of this case
study can be found on GitHub (https://github.com/paul-buerkner/LFO-CV-paper).

```{r}
M <- 1
lh_elpd_1sap_exact <- compute_lfo(
  fit_lh, type = "exact", M = M, L = L, 
  file = "results/lh_elpd_1sap_exact.rds"
)
lh_elpd_1sap_approx <- compute_lfo(
  fit_lh, type = "approx", M = M, L = L, 
  file = "results/lh_elpd_1sap_approx.rds"
)
refits <- attributes(lh_elpd_1sap_approx)$refits
nrefits <- length(refits)

sum_lh_elpd_1sap_exact <- summarize_elpds(lh_elpd_1sap_exact)[1]
sum_lh_elpd_1sap_approx <- summarize_elpds(lh_elpd_1sap_approx)[1]
```

```{r}
M <- 4
lh_elpd_4sap_exact <- compute_lfo(
  fit_lh, type = "exact", M = M, L = L, 
  file = "results/lh_elpd_4sap_exact.rds"
)
lh_elpd_4sap_approx <- compute_lfo(
  fit_lh, type = "approx", M = M, L = L, 
  file = "results/lh_elpd_4sap_approx.rds"
)
sum_lh_elpd_4sap_exact <- summarize_elpds(lh_elpd_4sap_exact)[1]
sum_lh_elpd_4sap_approx <- summarize_elpds(lh_elpd_4sap_approx)[1]
```

We start by computing exact and PSIS-approximated LFO-CV of 1-SAP. We compute 
${\rm ELPD}_{\rm exact} =$ `r fmt(sum_lh_elpd_1sap_exact, 1)` and 
${\rm ELPD}_{\rm approx} =$ `r fmt(sum_lh_elpd_1sap_approx, 1)`, which are
almost identical. Not only is the overall ELPD estimated accurately but also
each of the pointwise ELPD contributions (see the left panel of Figure
\@ref(fig:lh-pw-elpd)). In comparison, PSIS-LOO-CV returns
${\rm ELPD}_{\rm loo} =$ `r fmt(loo_lh$estimates[1, 1], 1)` and thus
overestimates the predictive performance, which coincides with our simulation
results of stationary autoregessive models (see fourth row of Figure
\@ref(fig:1sap)). Plotting the Pareto $k$ estimates reveals that the model had
to be refit `r nrefits` times, out of a total of $N - L =$ `r N - L` predicted
observations (see Figure \@ref(fig:lh-pareto-k)). On average, this means one
refit every `r fmt((N - L) / nrefits, 1)` observations, which implies a drastic
speed increase as compared to exact LFO-CV.

Performing LFO-CV of 4-SAP, we compute ${\rm ELPD}_{\rm exact} =$ 
`r fmt(sum_lh_elpd_4sap_exact, 1)` and ${\rm ELPD}_{\rm approx} =$ 
`r fmt(sum_lh_elpd_4sap_approx, 1)`, which are again almost identical.
In general, for increasing $M$, the approximation will tend to become more variable
around the true value in absolute ELPD units, as the ELPD increment of each 
observation will be based on more and more observations (see also Section 
\@ref(simulations)). For this example, we see some
differences in the pointwise ELPD contributions of specific observations which
were hard to predict accurately by the model (see the right panel of Figure
\@ref(fig:lh-pw-elpd)). However, these differences cancel out in
the overall ELPD estimate. Since, for constant threshold $\tau$, the importance
weights are the same independent of $M$, the Pareto $k$ estimates are also the 
same in $4$-SAP as in $1$-SAP.

```{r lh-pw-elpd, warning=FALSE, fig.height=3, fig.cap="Pointwise exact vs. PSIS-approximated ELPD contributions of 1-SAP (left) and 4-SAP (right) for the Lake Huron model using a threshold of $\\tau = 0.6$ for the Pareto $k$ estimates. $M$ = number of predicted future observations."}
lh_pw_elpd <- tibble(
  elpd_exact = na.omit(lh_elpd_1sap_exact),
  elpd_approx = na.omit(lh_elpd_1sap_approx),
  k = na.omit(attributes(lh_elpd_1sap_approx)$ks),
  M = "M = 1"
) %>% bind_rows(
  tibble(
    elpd_exact = na.omit(lh_elpd_4sap_exact),
    elpd_approx = na.omit(lh_elpd_4sap_approx),
    k = na.omit(attributes(lh_elpd_4sap_approx)$ks),
    M = "M = 4"
  )
)
ggplot(lh_pw_elpd, aes(elpd_exact, elpd_approx)) +
  facet_wrap(facets = "M", nrow = 1, ncol = 2, scales = "free") +
  geom_abline(slope = 1) +
  geom_point() +
  labs(y = "Approximate ELPD", x = "Exact ELPD") + 
  theme_bw()
```

```{r lh-pareto-k, fig.height=2.5, fig.cap="Pareto $k$ estimates for PSIS-LFO-CV of the Lake Huron model leaving out all future values. The dotted red line indicates the threshold at which the refitting was necessary."}
ks <- na.omit(attributes(lh_elpd_1sap_approx)$ks)
ids <- (L + 1):N
plot_ks(ks, ids)
```


## Annual date of the cherry blossoms in Japan {#case-CB}

```{r}
cherry <- read.csv("data/cherry_blossoms.csv")
cherry_temp <- cherry[!is.na(cherry$temp), ]
cherry_doy <- cherry[!is.na(cherry$doy), ]
```

The cherry blossom in Japan is a famous natural phenomenon occurring once every
year during spring. As climate changes so does the annual date of the cherry
blossom [@aono2008; @aono2010]. The most complete reconstruction available to
date contains data between `r min(cherry$year)` AD and `r max(cherry$year)` AD
[@aono2008; @aono2010]. The data is freely available online
(http://atmenv.envi.osakafu-u.ac.jp/aono/kyophenotemp4/).

In this case study, we are going to predict the annual date of the cherry
blossom using an approximate Gaussian process model [@solin2014, @RiutortMayol2019] to
provide flexible non-linear smoothing of the time-series. A visualisation of
both the data and the fitted model in provided in Figure
\@ref(fig:cherry-blossom). While the time-series appears rather stable across
earlier centuries, with substantial variation across consecutive years, there
are some clearly visible trends in the data. In particular in more recent years,
the cherry blossom tended to happen much earlier than before, presumably as a
result of climate change [@aono2008; @aono2010].

Based on this data and model, we will illustrate the use of PSIS-LFO-CV to
provide estimates of $1$-SAP and $4$-SAP leaving out all future values. To allow
for reasonable predictions of future values, we will require at least $L = 100$
historical observations (100 years) to make predictions. Further, we set a
threshold of $\tau =$ `r k_thres` for the Pareto $k$ estimates at which define that
refitting becomes necessary. Our fully reproducible analysis of this case study 
can be found on GitHub (https://github.com/paul-buerkner/LFO-CV-paper).

```{r fit_cb}
fit_cb <- brm(
  formula = bf(doy ~ gp(year, k = 20, c = 5/4)), 
  data = cherry_doy, 
  prior = prior(normal(0, 0.1), class = lscale, coef = gpyear),
  chain = 2, warmup = 4000, iter = 7000, inits = 0,
  control = list(adapt_delta = 0.99, max_treedepth = 15),
  seed = 5838234, file = "models/fit_cb"
)
```

```{r cherry-blossom, fig.height=3, fig.cap="Day of the cherry blossom in Japan (812-2015). Black points are observed data. The blue line represents mean predictions of a thin-plate spline model with 90% regression intervals shown in gray."}
me_cb <- marginal_effects(fit_cb, probs = c(0.05, 0.95))
plot(me_cb, points = TRUE, plot = FALSE)[[1]] +
  labs(x = "Year", y = "Day of cherry blossom")
```

```{r}
N <- NROW(cherry_doy)
L <- 100
M <- 1
cb_elpd_1sap_exact <- compute_lfo(
  fit_cb, type = "exact", M = M, L = L, 
  file = "results/cb_elpd_1sap_exact.rds"
)
cb_elpd_1sap_approx <- compute_lfo(
  fit_cb, type = "approx", M = M, L = L, 
  file = "results/cb_elpd_1sap_approx.rds"
)
refits <- attributes(cb_elpd_1sap_approx)$refits
nrefits <- length(refits)

sum_cb_elpd_1sap_exact <- summarize_elpds(cb_elpd_1sap_exact)[1]
sum_cb_elpd_1sap_approx <- summarize_elpds(cb_elpd_1sap_approx)[1]

loo_cb <- loo(fit_cb, newdata = cherry_doy[-seq_len(L), ])
```

```{r}
M <- 4
cb_elpd_4sap_exact <- compute_lfo(
  fit_cb, type = "exact", M = M, L = L, 
  file = "results/cb_elpd_4sap_exact.rds"
)
cb_elpd_4sap_approx <- compute_lfo(
  fit_cb, type = "approx", M = M, L = L, 
  file = "results/cb_elpd_4sap_approx.rds"
)

sum_cb_elpd_4sap_exact <- summarize_elpds(cb_elpd_4sap_exact)[1]
sum_cb_elpd_4sap_approx <- summarize_elpds(cb_elpd_4sap_approx)[1]
```

We start by computing exact and PSIS-approximated LFO-CV of 1-SAP. We compute
${\rm ELPD}_{\rm exact} =$ `r fmt(sum_cb_elpd_1sap_exact, 1)` and 
${\rm ELPD}_{\rm approx} =$ `r fmt(sum_cb_elpd_1sap_approx, 1)`, which are highly
similar. PSIS-LFO-CV slightly overestimates the predictive performance for 
$\tau =$ `r k_thres`, which is in line with our simulation results (see Section
\@ref(simulations)). However, as the difference is so small, it may also just be
random error. As shown in the left panel of Figure \@ref(fig:cb-pw-elpd), the
pointwise ELPD contributions are highly accurate, with no outliers, indicating
the our approximation has worked out consistently well across observations.
PSIS-LFO-CV clearly performs better than PSIS-LOO-CV for which we obtain ${\rm
ELPD}_{\rm exact} =$ `r fmt(loo_cb$estimates[1, 1], 1)` and thus an
overestimation of the predictive performance. Plotting the Pareto $k$ estimates
reveals that the model had to be refit `r nrefits` times, out of a total of $N -
L =$ `r N - L` predicted observations (see Figure \@ref(fig:cb-pareto-k)). On
average, this means one refit every `r fmt((N - L) / nrefits, 1)` observations,
which implies a drastic speed increase as compared to exact LFO-CV.

Performing LFO-CV of 4-SAP, we compute ${\rm ELPD}_{\rm exact} =$ 
`r fmt(sum_cb_elpd_4sap_exact, 1)` and ${\rm ELPD}_{\rm approx} =$ 
`r fmt(sum_cb_elpd_4sap_approx, 1)`, which are again similar but not as close as
the corresponding 1-SAP results. This is to be expected as the uncertainty of
PSIS-LFO-CV increases for increasing $M$ (see Section \@ref(simulations)). As
displayed in the right panel of Figure \@ref(fig:cb-pw-elpd), the pointwise 
ELPD contributions are highly accurate, with no outliers, indicating the our
approximation has worked out consistently well across observations. Since, for
constant threshold $\tau$, the importance weights are the same independent of
$M$, the Pareto $k$ estimates are also the same in $4$-SAP as in $1$-SAP.

```{r cb-pw-elpd, warning=FALSE, fig.height=3, fig.cap="Pointwise exact vs. PSIS-approximated ELPD contributions of 1-SAP (left) and 4-SAP (right) for the cherry blossom model using a threshold of $\\tau = 0.6$ for the Pareto $k$ estimates. $M$ = number of predicted future observations."}
cb_pw_elpd <- tibble(
  elpd_exact = na.omit(cb_elpd_1sap_exact),
  elpd_approx = na.omit(cb_elpd_1sap_approx),
  k = na.omit(attributes(cb_elpd_1sap_approx)$ks),
  M = "M = 1"
) %>% bind_rows(
  tibble(
    elpd_exact = na.omit(cb_elpd_4sap_exact),
    elpd_approx = na.omit(cb_elpd_4sap_approx),
    k = na.omit(attributes(cb_elpd_4sap_approx)$ks),
    M = "M = 4"
  )
)
ggplot(cb_pw_elpd, aes(elpd_exact, elpd_approx)) +
  facet_wrap(facets = "M", nrow = 1, ncol = 2, scales = "free") +
  geom_abline(slope = 1) +
  geom_point() +
  labs(y = "Approximate ELPD", x = "Exact ELPD") + 
  theme_bw()
```

```{r cb-pareto-k, fig.height=2.5, fig.cap="Pareto $k$ estimates for PSIS-LFO-CV of the cherry blossom model leaving out all future values. The dotted red line indicates the threshold at which the refitting was necessary."}
ks <- na.omit(attributes(cb_elpd_1sap_approx)$ks)
ids <- (L + 1):N
plot_ks(ks, ids)
```


# Conclusion {#discussion}

In the present paper, we proposed and evaluated a new method to approximate
cross-validation methods for time-series models, which we called PSIS-LFO-CV. It
follows the common task of time-series models to predict future values based
solely on past values. Within the set of such prediction tasks, we can choose
the number $M$ of future values to be predicted at a time. In theory, we may
also choose how much of the future we leave out, either all future values
($M$-SAP) or only a block of more recent future values (block-$M$-SAP). However,
for reasons discussed in Appendix B, we do not recommend using
block-$M$-SAP in practice.

For a set of common time-series models, we established via simulations that
PSIS-LFO-CV is an almost unbiased approximation of exact LFO-CV if we choose the
threshold $\tau$ of the Pareto $k$ estimates to be not larger than $\tau = 0.6$.
As the number of required model refits, and thus the computational time,
increases with decreasing $\tau$, we currently see $\tau = 0.6$ as a good
default when performing PSIS-LFO-CV. This is noticeably smaller than the
recommended threshold for PSIS-LOO-CV of $\tau = 0.7$, because, in PSIS-LFO-CV,
the errors are dependent as highly influential observations also influence the
approximation in the following iterations before refit, thus having a stronger
influence on the overall accuracy than in PSIS-LOO-CV.

Lastly, we want to briefly note that LFO-CV can also be used to compute marginal
likelihoods. Using basic rules of conditional probability, we can factorize the
log marginal likelihood as

\begin{equation}
\log p(y) = \sum_{i=1}^N \log p(y_i \,|\, y_{1:i}).
\end{equation}

This is nothing else than the ELPD of 1-SAP if we set $L = 0$, that is if we
choose to predict *all* observations using their respective past (the very
first observation is only predicted from the prior). As such, marginal
likelihoods may be approximated using PSIS-LFO-CV. Although this approach is
unlikely to be more efficient than methods specialized to compute marginal
likelihoods such as bridge sampling [@meng1996; @meng2002; @gronau2017], it may
be a noteworthy options if, for some reason, other methods fail.

# Acknowledgments

We thank Daniel Simpson, Shira Mitchell, and M\r{a}ns Magnusson for helpful 
comments and discussions on earlier versions of this paper.

\newpage

# Appendix {-}

## Appendix A: Pseudo code of PSIS LFO-CV {-}

The R flavored pseudo code shown below provides a description of the proposed
PSIS LFO-CV algorithm when leaving out all future values. The actual R code can
be found on GitHub (https://github.com/paul-buerkner/LFO-CV-paper).

```{r, tidy=FALSE, eval=FALSE, echo = TRUE}
PSIS_LFO_CV = function(model, data, M, L, tau) {
  # Arguments:
  #   model: the fitted time-series model based on the complete data
  #   data: the complete data set
  #   M: number of steps to be predicted into the future
  #   L: minimal number of observations necessary to make predictions
  #   tau: threshold of the Pareto-k-values
  # Returns:
  #   PSIS approximated ELPD value when leaving out all future values
  N = number_of_rows(data)
  S = number_of_draws(model)
  LL_matrix = matrix(nrow = S, ncol = N)
  out = vector(length = N)
  model_refit = model
  i_star = N
  for (i in (N - M + 1):(L + 1)) {
    LL = log_likelihood(model_refit, data = data[1:(i + M - 1), ])
    LL_matrix[, i] = LL[, i]
    PSIS_object = PSIS(-sum_per_row(LL_matrix[, i:i_star]))
    k = pareto_k_values(PSIS_object)
    if (k > tau) {
      # refitting the model is necessary
      i_star = i - 1
      model_refit = update(model_refit, data = data[1:(i - 1), ])
      # perform exact LFO for the ith observation
      LL = log_likelihood(model_refit, data = data[1:(i + M - 1), ])
      loglik[, i] = LL[, i]
      out[i] = log_mean_exp(sum_per_row(LL[, i:(i + M - 1)]))
    } else {
      # PSIS approximation is possible
      LW = log_weights(PSIS_object)
      out[i] = log_sum_exp(LW + sum_per_row(LL[, i:(i + M - 1)]))
    }
  }
  return(sum(out))
}
```

\newpage

## Appendix B: Block $M$-step-ahead predictions {-#approximate-blockMSAP}

Depending on the particular time-series data and model, the Pareto $k$ estimates
may exceed $\tau$ rather quickly (i.e., after only few observations) and so
a lot of refits may be required even when carrying out the PSIS approximation
to LFO-CV. In this case, another option is to exclude only the block of $B$ 
future values that directly follow the observations to be predicted while 
retaining all of the more distant values $y_{(i+B):N} = (y_{i + B}, \ldots, y_N)$. 
This will usually result in lower Pareto $k$ estimates and thus less refitting,
but crucially alters the underlying prediction task, to which we will refer
to as block-$M$-SAP.

The block-$M$-SAP version closely resembles the basic $M$-SAP only if values in
the distant future, $y_{(i+B):N}$, contain little information about the current
observations $i$ being predicted, apart from just increasing precision of the 
estimated global parameters. Whether this assumption is justified will depend
on the data and model. That is, if the time-series is non-stationary, distant
future value will inform overall trends in the data and thus clearly inform
predictions of the current observations being left-out. As a result, 
block-LFO-CV is only recommended for stationary time-series and corresponding
models.

There are more complexities that arise in block-$M$-SAP that we did not have to
care about in standard $M$-SAP. One is that, by just removing the block, the
time-series effectively gets split into two parts, one before and one after the
block. This poses no problem for conditionally independent time-series models,
where predictions just depend on the parameters and not on the former values of
the time-series itself. However, if the model's predictions are *not*
conditionally independent as is the case, for instance, in autoregressive models
(see Section \@ref(simulations)), the observations of the left-out block have to
be modeled as missing values in order to retain the integrity of the
time-series' predictions after the block. A related example from spatial
statistics, in which the modeling of missing values is required for valid
inference, can be found in @buerkner:non-factorizable.

Another complexity concerns the PSIS approximation of block-LFO-CV: Not only
does the approximating model contain more observations than the current model
whose predictions we are approximating, but it also may *not* contain
observations that are present in the actual model. The latter observations are
those right after the currently left-out block, which are included in the
current model, but not in the approximating model as they were part of the block
at the time the approximating model was (re-)fit. A visualisation of this
situation is provided in Figure \@ref(fig:vis-block-msap). More formally, let
$\overline{J}_i$ be the index set of observations that are missing in the
approximating model at the time of predicting observation $i$. We find

\begin{equation}
\overline{J}_i = \{ \max(i + B, N^\star + 1), \ldots, \min(N^\star + B, N) \}
\end{equation}

if $\max(i + B, N^\star + 1) \leq \min(N^\star + B, N)$ and 
$\overline{J}_i = \emptyset$ otherwise. As above, $N^\star$ refers to the 
largest observation included in the model fitting, that is 
$N^\star = i^\star - 1$ where $i^*$ is the index of the latest refit. The raw 
importance ratios $r_i^{(s)}$ for each posterior draw $s$ are then computed as

\begin{equation}
r_i^{(s)} \propto \frac{\prod_{j \in \overline{J}_i} p(y_j \,|\, \,\theta^{(s)})}
{\prod_{j \in J_i} p(y_j \,|\, \,\theta^{(s)})}
\end{equation}

before they are stabilized and further processed using PSIS (see Section
\@ref(approximate-MSAP)).

```{r vis-block-msap, fig.width=8, fig.height=3, fig.cap="Visualisation of PSIS approximated one-step-ahead predictions leaving out a block of $B = 3$ future values. Predicted observations are indicated by **X**. Observation in the left out block are indicated by **B**. In the shown example, the model was last refit at the $i^\\star = 5$th observation."}
status_levels <- c("included", "included (PSIS)", "left out", "left out (PSIS)")
df <- data.frame(
  obs = rep(1:9, 3),
  i = factor(rep(3:5, each = 9)),
  Status = c(
    rep("included", 2), rep("left out (PSIS)", 2), rep("left out", 2), 
    rep("included (PSIS)", 2), rep("included", 1),
    rep("included", 3), rep("left out (PSIS)", 1), rep("left out", 3), 
    rep("included (PSIS)", 1), rep("included", 1),
    rep("included", 4), rep("left out", 4), rep("included", 1)
  )
) %>%
  mutate(Status = factor(Status, levels = status_levels))

block_msap_colors <- c(
  bayesplot::color_scheme_get("viridis")$light,
  bayesplot::color_scheme_get("viridis")$light_highlight,
  bayesplot::color_scheme_get("viridis")$dark,
  bayesplot::color_scheme_get("viridis")$mid_highlight
)

ggplot(df, aes(obs, i, fill = Status)) +
  geom_tile(height = 0.9, width = 1, col = "black") +
  annotate(
    'text', x = 3:5, y = 1:3, 
    label = "X", parse = TRUE, 
    size = 10, color = "white"
  ) +
  annotate(
    'text', x = c(4:6, 5:7, 6:8), y = rep(1:3, each = 3), 
    label = "B", parse = TRUE, size = 10, color = "white"
  ) +
  labs(x = "Observation", y = "Predicted observation") +
  scale_x_continuous(breaks = 1:9) +
  scale_fill_manual(values = block_msap_colors) +
  bayesplot::theme_default() +
  NULL
```

### Simulations {-}

In the simulation of block-$M$-SAP, we use the same conditions as for 
ordinary $M$-SAP, but instead of leaving out all future values, we
left out a block of only $B = 10$ future values. 

```{r}
N <- length(LakeHuron)
L <- 20
B <- 10
M <- 1
lh_elpd_block1sap_exact <- compute_lfo(
  fit_lh, type = "exact", M = M, L = L, B = B,
  file = "results/lh_elpd_block1sap_exact.rds"
)
lh_elpd_block1sap_approx <- compute_lfo(
  fit_lh, type = "approx", M = M, L = L, B = B,
  file = "results/lh_elpd_block1sap_approx.rds"
)

refits <- attributes(lh_elpd_block1sap_approx)$refits
nrefits <- length(refits)

sum_lh_elpd_block1sap_exact <- summarize_elpds(lh_elpd_block1sap_exact)[1]
sum_lh_elpd_block1sap_approx <- summarize_elpds(lh_elpd_block1sap_approx)[1]
```

```{r}
M <- 4
lh_elpd_block4sap_exact <- compute_lfo(
  fit_lh, type = "exact", M = M, L = L, B = B,
  file = "results/lh_elpd_block4sap_exact.rds"
)
lh_elpd_block4sap_approx <- compute_lfo(
  fit_lh, type = "approx", M = M, L = L, B = B,
  file = "results/lh_elpd_block4sap_approx.rds"
)
sum_lh_elpd_block4sap_exact <- summarize_elpds(lh_elpd_block4sap_exact)[1]
sum_lh_elpd_block4sap_approx <- summarize_elpds(lh_elpd_block4sap_approx)[1]
```

Results of the block-1-SAP simulations are shown in Figure \@ref(fig:block1sap).
PSIS-LFO-CV provides almost unbiased estimate of the corresponding exact LFO-CV
for all investigated conditions, that is regardless of the threshold $\tau$ or
the data generating model. The number of required refits was not only much
smaller than when leaving out all future values, but practically approached zero
for most conditions (see Table \@ref(tab:block-refits)). PSIS-LOO-CV has also
small bias, but higher variance than PSIS-LFO-CV. This is plausible given that
LOO-CV and LFO-CV of block-1-SAP only differ in whether they include the
relatively few observations in the block when fitting the approximating model.

```{r block-refits, cache=FALSE}
lfo_sims %>% 
  filter(!is.na(B)) %>%
  select(model, M, k_thres, rel_nrefits) %>%
  group_by(model, M, k_thres,) %>%
  summarise(rel_nrefits = round(mean(rel_nrefits), 2)) %>%
  ungroup() %>%
  spread("model", "rel_nrefits") %>%
  mutate(M = ifelse(duplicated(M), "", M)) %>%
  rename(`$\\tau$` = "k_thres") %>%
  kable(
    caption = "Mean proportions of required refits for block-$M$-SAP.",
    booktabs = TRUE,
    escape = FALSE
  ) %>%
  footnote(
    general = "Note: Results are based on 100 simulation trials of time-series with $N = 200$ observations requiring at least $L = 25$ observations to make predictions. The number of left-out future observations was set to $B = 10$. Abbreviations: $\\\\tau$ = threshold of the Pareto $k$ estimates; $M$ = number of predicted future observations.",
    general_title = "",
    threeparttable = TRUE,
    escape = FALSE
  )
```

```{r block1sap, fig.height=8, fig.cap="Simulation results of block 1-step-ahead predictions. Histograms are based on 100 simulation trials of time-series with $N = 200$ observations requiring at least $L = 25$ observations to make predictions. The number of left-out future observations was set to $B = 10$."}
block_lfo_sims_1sap %>% 
  select(elpd_diff_lfo, elpd_diff_loo, model, tau) %>%
  gather("Type", "elpd_diff", elpd_diff_lfo, elpd_diff_loo) %>%
  ggplot(aes(x = elpd_diff, y = ..density.., fill = Type)) +
  facet_grid(
    model ~ tau, scales = "free_y", 
    labeller = label_parsed
  ) +
  geom_histogram(alpha = 0.7) +
  scale_fill_manual(
    values = colors,
    labels = c("Approximate LFO-CV", "Approximate LOO-CV")
  ) +
  labs(x = 'ELPD difference to exact block-LFO-CV', y = "Density") +
  geom_vline(xintercept = 0, linetype = 2) +
  theme_bw() +
  theme(legend.position = "bottom") +
  NULL
```

Results of the block-4-SAP simulations (see Figure \@ref(fig:block4sap)) are
overall similar to the corresponding block-1-SAP simulations. In particular,
PSIS-LFO-CV has small bias compared to the exact LFO-CV. However, the accuracy
of PSIS-LFO-CV for block-4-SAP is highly variable when applied to autoregressive
models (see Figure \@ref(fig:block4sap)), something that
is also visible in block-1-SAP although to a smaller degree. This seems to be a
counter-intuitive result given that predictions should be more certain in the
block version as more observations are available to inform the model. However,
it can be explained as follows. In autoregressive models, predictions of future
observations directly depend on past observations, that is predictions are not
conditionally independent. This becomes a problem when dealing with observations
that are missing in the approximating model right after the block of left out
observations, since the directly preceding observations are part of the block
and are thus have to be treated as missing values (for details see Section
\@ref(approximate-blockMSAP)). This implies a disproportionally high variability
in the predictions of observations right after the block in autoregressive
models, which then naturally propagates into higher variability of the
PSIS-LFO-CV approximations.

```{r block4sap, fig.height=8, fig.cap="Simulation results of block 4-step-ahead predictions. Histograms are based on 100 simulation trials of time-series with $N = 200$ observations requiring at least $L = 25$ observations to make predictions. The number of left-out future observations was set to $B = 10$."}
block_lfo_sims_4sap %>% 
  select(elpd_diff_lfo, model, tau) %>%
  ggplot(aes(x = elpd_diff_lfo, y = ..density..)) +
  facet_grid(
    model ~ tau, scales = "free_y", 
    labeller = label_parsed
  ) +
  geom_histogram(alpha = 0.7, fill = colors[1]) +
  labs(x = 'ELPD difference of approximate and exact block-LFO-CV', y = "Density") +
  geom_vline(xintercept = 0, linetype = 2) +
  theme_bw() +
  theme(legend.position = "bottom") +
  NULL
```

### Annual measurements of the level of Lake Huron {-}

In the following, we discuss the application of block-LFO-CV on our case study
about annual measurements of the level of Lake Huron (see Section
\@ref(case-LH)). It is not entirely clear how stationary the time-series is as it
may have a slight negative trend across time (see Figure \@ref(fig:lake-huron)).
However, the AR(4) model we are using assumes stationarity and it is appropriate
to also use block-LFO-CV for this example, at least for illustration. We choose
to leave out a block of $B = 10$ future values as the dependency of an AR(4)
model will not reach that far into the future. That is, we will include all
observations after this block when re-fitting the model.

Approximate LFO-CV of block-1-SAP reveals 
${\rm ELPD}_{\rm exact} =$ `r fmt(sum_lh_elpd_block1sap_exact, 1)` and 
${\rm ELPD}_{\rm approx} =$ `r fmt(sum_lh_elpd_block1sap_approx, 1)`, 
which are almost identical. Plotting the Pareto $k$ estimates reveals that the
model had to be refit `r nrefits` times, out of a total of $N - L =$ 
`r N - L` predicted observations (see Figure \@ref(fig:lh-pareto-k-block)). On average, 
this means one refit every `r fmt((N - L) / nrefits, 1)` observations, which
again implies a drastic speed increase as compared to exact LFO-CV. What is more,
we needed even fewer refits than in non-block LFO-CV, an observation we already
made in our simulation in Section \@ref(simulations).
Performing LFO-CV of block-4-SAP, we compute
${\rm ELPD}_{\rm exact} =$ `r fmt(sum_lh_elpd_block4sap_exact, 1)` and 
${\rm ELPD}_{\rm approx} =$ `r fmt(sum_lh_elpd_block4sap_approx, 1)`, 
which are similar but not quite a close as in the 1-SAP case.
Since AR-models fall in the class of conditionally dependent models, predicting
observations right after the left-out block may be quite difficult as shown
in Section \@ref(simulations). However, for the present data set, the
PSIS approximations of block-LFO-CV seem to have worked reasonably well.

```{r lh-pareto-k-block, fig.height=2.5, fig.cap="Pareto $k$ estimates for PSIS-LFO-CV of the Lake Huron model leaving out a block of 10 future values. The dotted red line indicates the threshold at which the refitting was necessary."}
ks <- na.omit(attributes(lh_elpd_block1sap_approx)$ks)
ids <- (L + 1):N
plot_ks(ks, ids)
```

### Conclusion {-}

Among other things, our simulations indicated that the accuracy of PSIS
approximated block-$M$-SAP is highly variable for conditionally dependent models
such as autoregressive models. Together with the fact that block-$M$-SAP is only
theoretically reasonable for stationary time series, as the future will always
be informative for non-stationary ones, this leaves PSIS approximated
block-$M$-SAP in a difficult spot. It appears to be a theoretically reasonable
and empirically accurate choice only for conditionally independent models fit to
stationary time-series. If the time-series is not too long and the corresponding
model not too complex, so that a few more refits are acceptable, it might thus
be more consistent and safe to just use PSIS-LFO-CV of $M$-SAP not trying to
approximate block-$M$-SAP at all.
